{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd40f4f4-5a80-40b7-94b3-aed136daa9d8",
   "metadata": {},
   "source": [
    "# Evaluate the Impact of Population-Weighting on Heat Wave Events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e597e9-538f-41d4-afa9-a268b10ab1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing the packages we need:\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f6fe5-0769-4298-887e-7c4cffc64a0d",
   "metadata": {},
   "source": [
    "## Set the Directory Structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd887600-32da-46dc-b411-fa63c75f564f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Identify the data input and image output directories:\n",
    "service_territory_data_dir = '/Users/burl878/Documents/IMMM/Data/TELL/Production_Runs/tell_data/ba_service_territory_data/'\n",
    "population_data_dir = '/Users/burl878/Documents/IMMM/Data/TELL/Production_Runs/tell_data/population_data/'\n",
    "weather_data_dir =  '/Users/burl878/Documents/IMMM/Data/TELL/Production_Runs/tell_data/wrf_tell_counties_output/historic/'\n",
    "load_data_dir = '/Users/burl878/Documents/IMMM/Data/TELL/Production_Runs/tell_data/outputs/mlp_output/historic/'\n",
    "data_output_dir =  '/Users/burl878/Documents/Code/code_repos/nerc_analysis/data/'\n",
    "image_output_dir =  '/Users/burl878/Documents/Code/code_repos/nerc_analysis/plots/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7a320d-804a-44c4-b415-da9450d37344",
   "metadata": {},
   "source": [
    "## Process the Weather and Load Time Series by BA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77098b53-9385-4a75-92af-f106f7715804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to process the load time series for a given BA and date range:\n",
    "def process_ba_load_time_series(ba_to_process: str, start_year: int, end_year: int, load_data_dir: str):\n",
    "    \n",
    "    # Loop over the years of load data:\n",
    "    for year in range(start_year, end_year, 1):\n",
    "        \n",
    "        # Read in the .csv file and replace missing values with nan:\n",
    "        mlp_data = pd.read_csv((load_data_dir + '/' + str(year) + '/' + ba_to_process + '_' + str(year) + '_mlp_output.csv')).replace(-9999, np.nan)\n",
    "\n",
    "        # Set the time variable as a datetime variable:\n",
    "        mlp_data['Time_UTC'] = pd.to_datetime(mlp_data['Time_UTC'])\n",
    "        \n",
    "        # Rename the \"BA\" variable:\n",
    "        mlp_data.rename(columns={'BA': 'BA_Code'}, inplace=True)\n",
    "\n",
    "        # Rename the \"Load\" variable:\n",
    "        mlp_data.rename(columns={'Load': 'Load_MWh'}, inplace=True)\n",
    "\n",
    "        # Replacing missing or negative loads with NaN:\n",
    "        mlp_data.loc[~(mlp_data['Load_MWh'] > 0), 'Load_MWh'] = np.nan\n",
    "\n",
    "        # Subset to just the variables we need:\n",
    "        mlp_data = mlp_data[['Time_UTC', 'Load_MWh']]\n",
    "    \n",
    "        # Aggregate the output into a new dataframe:\n",
    "        if year == start_year:\n",
    "           mlp_output_df = mlp_data\n",
    "        else:\n",
    "           mlp_output_df = pd.concat([mlp_output_df, mlp_data])\n",
    "        \n",
    "    return mlp_output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f141a5-13fe-436e-a241-1a7dd3808314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process the time series for a given BA and date range:\n",
    "def process_ba_time_series(ba_to_process: str, start_year: int, end_year: int, weather_data_dir: str, service_territory_data_dir: str, \n",
    "                           population_data_dir: str, load_data_dir: str, data_output_dir: str):\n",
    "    \n",
    "    # Read in the county-level population data:\n",
    "    pop_df = pd.read_csv(population_data_dir + 'county_populations_2000_to_2020.csv')\n",
    "\n",
    "    # Subset to just the variables we need:\n",
    "    pop_df = pop_df[['county_FIPS', 'pop_2019']]\n",
    "\n",
    "    # Rename the variables for simplicity:\n",
    "    pop_df.rename(columns={'county_FIPS': 'FIPS', 'pop_2019': 'Population'}, inplace=True)\n",
    "    \n",
    "    # Read in the BA-to-county mapping file:\n",
    "    mapping_df = pd.read_csv(service_territory_data_dir + 'ba_service_territory_2019.csv')\n",
    "    \n",
    "    # Subset to just the BA you want to process:\n",
    "    mapping_df = mapping_df.loc[(mapping_df['BA_Code'] == ba_to_process)]\n",
    "    \n",
    "    # Rename the variables for simplicity:\n",
    "    mapping_df.rename(columns={'County_FIPS': 'FIPS'}, inplace=True)\n",
    "    \n",
    "    # Subset to just the variables we need:\n",
    "    mapping_df = mapping_df[['BA_Code', 'FIPS']]\n",
    "    \n",
    "    # Initiate a counter to store the results:\n",
    "    counter = 0;\n",
    "    output_df = pd.DataFrame()\n",
    "    \n",
    "    # Loop over the years of weather data:\n",
    "    for year in range(start_year, end_year, 1):\n",
    "        \n",
    "        # Print the year\n",
    "        print(str(year))\n",
    "        \n",
    "        # Create a list of all county meteorology files in the input directory:\n",
    "        list_of_files = glob(os.path.join(weather_data_dir, str(year), '*.csv'))\n",
    "    \n",
    "        # Loop over that list process each file:\n",
    "        for file in range(len(list_of_files)):\n",
    "        # for file in range(1):\n",
    "            # Iterate the counter by one:\n",
    "            counter = counter + 1\n",
    "        \n",
    "            # Extract the filename from the list:\n",
    "            filename = list_of_files[file].rsplit('/', 1)[1]\n",
    "       \n",
    "            # Extract the time string from the name of the file:\n",
    "            filetime = filename.replace(\"_UTC_County_Mean_Meteorology.csv\", \"\")\n",
    "            \n",
    "            # Read in the .csv file:\n",
    "            met_df = pd.read_csv(list_of_files[file])\n",
    "            \n",
    "            # Convert the temperature from Kelvin to Fahrenheit:\n",
    "            met_df['T2'] = (1.8 * (met_df['T2'] - 273)) + 32\n",
    "        \n",
    "            # Merge the meteorology and population data into the mapping_df\n",
    "            ba_df = mapping_df.merge(met_df, on=['FIPS']).merge(pop_df, on=['FIPS'])\n",
    "        \n",
    "            # Compute the fraction of the total population in the BA that lives in a given county:\n",
    "            ba_df['Population_Fraction'] = ba_df['Population'] / (ba_df['Population'].sum())\n",
    "\n",
    "            # Population-weight T2:\n",
    "            ba_df['T2_Weighted'] = (ba_df['T2'].mul(ba_df['Population_Fraction']))\n",
    "       \n",
    "            # Add the time step to the output file:\n",
    "            output_df.loc[counter, 'Time_UTC'] = pd.to_datetime(filetime, exact=False, format='%Y_%m_%d_%H')\n",
    "            output_df.loc[counter, 'T2_UW'] = (ba_df['T2'].mean()).round(2)\n",
    "            output_df.loc[counter, 'T2_PW'] = (ba_df['T2_Weighted'].sum()).round(2)\n",
    "            output_df.loc[counter, 'T2_Min'] = ba_df['T2'].min().round(2)\n",
    "            output_df.loc[counter, 'T2_Max'] = ba_df['T2'].max().round(2)\n",
    "            \n",
    "            # Clean up the old dataframes and move to the next file in the loop:\n",
    "            del filename, filetime, met_df, ba_df\n",
    "        \n",
    "    # Sort by time:\n",
    "    output_df = output_df.sort_values(['Time_UTC'])\n",
    "    \n",
    "    # Aggregate the TELL MLP output for the BA and date range:\n",
    "    load_df = process_ba_load_time_series(ba_to_process = ba_to_process, \n",
    "                                          start_year = start_year, \n",
    "                                          end_year = end_year, \n",
    "                                          load_data_dir = load_data_dir)\n",
    "    \n",
    "    # Merge the meteorology and load data:\n",
    "    output_df = output_df.merge(load_df, on=['Time_UTC'])\n",
    "        \n",
    "    # Create the ouput filename:    \n",
    "    csv_output_filename = os.path.join(data_output_dir, (ba_to_process + '_Time_Series_' + str(start_year) + '_to_' + str(end_year) + '.csv'))\n",
    "        \n",
    "    # Write out the dataframe to a .csv file:\n",
    "    output_df.to_csv(csv_output_filename, sep=',', index=False)\n",
    "    \n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e41ca9-a081-4ffb-92d2-045c965423a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df = process_ba_time_series(ba_to_process = 'CISO', \n",
    "                                   start_year = 1980, \n",
    "                                   end_year = 2020, \n",
    "                                   weather_data_dir = weather_data_dir, \n",
    "                                   service_territory_data_dir = service_territory_data_dir, \n",
    "                                   population_data_dir = population_data_dir, \n",
    "                                   load_data_dir = load_data_dir, \n",
    "                                   data_output_dir = data_output_dir)\n",
    "\n",
    "output_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04fca80-914f-47d2-b041-8b1fa5d9029c",
   "metadata": {},
   "source": [
    "## Classify Heat Wave Events Based on Historical Temperatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe483188-99e4-4757-8a16-465e93c9a801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to classify heat wave events based on one criteria:\n",
    "#  1) Daily maximum temperature exceeds the 95th percentile of temperature from a given range of base years for two or more days\n",
    "\n",
    "def process_heat_wave_time_series(ba_to_process: str, start_year: int, end_year: int, base_year_start: int, base_year_end: int,\n",
    "                                  weather_data_dir: str, service_territory_data_dir: str, \n",
    "                                  population_data_dir: str, load_data_dir: str, data_output_dir: str):\n",
    "    \n",
    "    # Check to see if the output already exist and if not then process it:\n",
    "    if os.path.isfile(os.path.join(data_output_dir, (ba_to_process + '_Time_Series_' + str(start_year) + '_to_' + str(end_year) + '.csv'))) == True:\n",
    "       # Load in the pre-processed data:\n",
    "       met_df = pd.read_csv(os.path.join(data_output_dir, (ba_to_process + '_Time_Series_' + str(start_year) + '_to_' + str(end_year) + '.csv')))\n",
    "    else:\n",
    "       met_df = process_ba_time_series(ba_to_process = ba_to_process, \n",
    "                                       start_year = start_year, \n",
    "                                       end_year = end_year, \n",
    "                                       weather_data_dir = weather_data_dir, \n",
    "                                       service_territory_data_dir = service_territory_data_dir, \n",
    "                                       population_data_dir = population_data_dir, \n",
    "                                       load_data_dir = load_data_dir, \n",
    "                                       data_output_dir = data_output_dir)\n",
    "    \n",
    "    # Make a copy of the dataframe for later:\n",
    "    output_df = met_df.copy()\n",
    "    \n",
    "    # Set the time variable as an index:\n",
    "    met_df.index = pd.to_datetime(met_df['Time_UTC'])\n",
    "        \n",
    "    # Compute the daily minimum and maximum temperature using resampling:\n",
    "    uw_df = met_df.resample('D')['T2_UW'].agg(['min', 'max']).reset_index()\n",
    "    pw_df = met_df.resample('D')['T2_PW'].agg(['min', 'max']).reset_index()\n",
    "        \n",
    "    # Rename the variables for consistency:\n",
    "    uw_df.rename(columns={'Time_UTC': 'Day', 'min': 'T2_UW_Min', 'max': 'T2_UW_Max'}, inplace=True)      \n",
    "    pw_df.rename(columns={'Time_UTC': 'Day', 'min': 'T2_PW_Min', 'max': 'T2_PW_Max'}, inplace=True)      \n",
    "   \n",
    "    # Merge the two dataframes together based on common days:\n",
    "    daily_df = uw_df.merge(pw_df, on=['Day'])\n",
    "    \n",
    "    # Add a column with the year values to be used in grouping:\n",
    "    daily_df['Year'] = daily_df['Day'].dt.year\n",
    "        \n",
    "    # Subset the daily data to just the base year to calculate extreme temperature thresholds:\n",
    "    daily_df_subset = daily_df.loc[(daily_df['Year'] >= base_year_start) & (daily_df['Year'] <= base_year_end)]\n",
    "    \n",
    "    # Calculate the temperature thresholds based on the 95th percentile of historical daily maximum temperature:\n",
    "    uw_t_threshold = daily_df_subset['T2_UW_Max'].quantile(0.95).round(2)\n",
    "    pw_t_threshold = daily_df_subset['T2_PW_Max'].quantile(0.95).round(2)\n",
    "    \n",
    "    # Print the values:\n",
    "    print(('Unweighted Temperature Threshold = ' + str(uw_t_threshold) + 'F'))\n",
    "    print(('Population-Weighted Temperature Threshold = ' + str(pw_t_threshold) + 'F'))\n",
    "    \n",
    "    # Create some heat wave index variables:\n",
    "    daily_df['UW_Heat_Wave_Initial'] = 0\n",
    "    daily_df['PW_Heat_Wave_Initial'] = 0\n",
    "    daily_df['UW_Heat_Wave_Final'] = 0\n",
    "    daily_df['PW_Heat_Wave_Final'] = 0\n",
    "    daily_df['Joint_Heat_Wave_Final'] = 0\n",
    "    \n",
    "    # Mark the heat waves based on days that exceed the temperature threshold:\n",
    "    daily_df.loc[daily_df['T2_UW_Max'] >= uw_t_threshold, 'UW_Heat_Wave_Initial'] = 1\n",
    "    daily_df.loc[daily_df['T2_PW_Max'] >= pw_t_threshold, 'PW_Heat_Wave_Initial'] = 1\n",
    "    \n",
    "    # Loop through the dataframe and check to see if consecutive days exceed the temperature threshold:\n",
    "    #daily_df['Test'] = (daily_df['UW_Heat_Wave_Initial'].diff(1)).astype('int')\n",
    "    for row in range(1,(len(daily_df)-1)):\n",
    "        if (((daily_df.loc[row, 'UW_Heat_Wave_Initial'] == 1) and (daily_df.loc[(row+1), 'UW_Heat_Wave_Initial'] == 1)) | \n",
    "            ((daily_df.loc[row, 'UW_Heat_Wave_Initial'] == 1) and (daily_df.loc[(row-1), 'UW_Heat_Wave_Initial'] == 1))):\n",
    "           daily_df.loc[row, 'UW_Heat_Wave_Final'] = 1\n",
    "        if (((daily_df.loc[row, 'PW_Heat_Wave_Initial'] == 1) and (daily_df.loc[(row+1), 'PW_Heat_Wave_Initial'] == 1)) | \n",
    "            ((daily_df.loc[row, 'PW_Heat_Wave_Initial'] == 1) and (daily_df.loc[(row-1), 'PW_Heat_Wave_Initial'] == 1))):\n",
    "           daily_df.loc[row, 'PW_Heat_Wave_Final'] = 1\n",
    "        if (daily_df.loc[row, 'UW_Heat_Wave_Final'] == 1) and (daily_df.loc[row, 'PW_Heat_Wave_Final'] == 1):\n",
    "           daily_df.loc[row, 'Joint_Heat_Wave_Final'] = 1\n",
    "    \n",
    "    # Rename the variables for simplicity:\n",
    "    daily_df.rename(columns={'UW_Heat_Wave_Final': 'UW_HW', 'PW_Heat_Wave_Final': 'PW_HW', 'Joint_Heat_Wave_Final': 'Joint_HW'}, inplace=True)\n",
    "    \n",
    "    # Subset to just the variables we need:\n",
    "    daily_df = daily_df[['Day', 'UW_HW', 'PW_HW', 'Joint_HW']]\n",
    "    \n",
    "    # Set the time variable as a datetime variable:\n",
    "    daily_df['Date'] = pd.to_datetime(daily_df['Day'])\n",
    "    daily_df['Day'] = daily_df['Date'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Set the time variable as an index:\n",
    "    output_df['Time_UTC'] = pd.to_datetime(output_df['Time_UTC'])\n",
    "    \n",
    "    # Reset the index:\n",
    "    output_df.reset_index()\n",
    "    \n",
    "    # Add a column with the day values to be used in grouping:\n",
    "    output_df['Day'] = output_df['Time_UTC'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Merge the two dataframes together based on common days:\n",
    "    output_df = output_df.merge(daily_df, on=['Day'])\n",
    "    \n",
    "    # Subset to just the variables we need:\n",
    "    output_df = output_df[['Time_UTC', 'T2_UW', 'T2_PW', 'T2_Min', 'T2_Max', 'Load_MWh', 'UW_HW', 'PW_HW', 'Joint_HW']]\n",
    "    \n",
    "    # Create the ouput filename:    \n",
    "    csv_output_filename = os.path.join(data_output_dir, (ba_to_process + '_Heat_Wave_Time_Series_' + str(start_year) + '_to_' + str(end_year) + '.csv'))\n",
    "        \n",
    "    # Write out the dataframe to a .csv file:\n",
    "    output_df.to_csv(csv_output_filename, sep=',', index=False)\n",
    "    \n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4bb2b2-ad20-4f37-b3e2-f387712f0d35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df = process_heat_wave_time_series(ba_to_process = 'ERCO', \n",
    "                                          start_year = 1980, \n",
    "                                          end_year = 2020, \n",
    "                                          base_year_start = 1980,\n",
    "                                          base_year_end = 1990,\n",
    "                                          weather_data_dir = weather_data_dir, \n",
    "                                          service_territory_data_dir = service_territory_data_dir, \n",
    "                                          population_data_dir = population_data_dir, \n",
    "                                          load_data_dir = load_data_dir, \n",
    "                                          data_output_dir = data_output_dir)\n",
    "\n",
    "output_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b01e46-059f-4ee1-b268-e8fff0034631",
   "metadata": {},
   "source": [
    "## Make the Plots to Characterize the Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859ad948-f11d-4ff7-a2af-6f17baf6e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heat_wave_frequency(ba_to_process: str, start_year: int, end_year: int, base_year_start: int, base_year_end: int,\n",
    "                             weather_data_dir: str, service_territory_data_dir: str, \n",
    "                             population_data_dir: str, load_data_dir: str, data_output_dir: str,\n",
    "                             image_output_dir: str, image_resolution: int, save_images=False):\n",
    "    \n",
    "    # Check to see if the output already exist and if not then process it:\n",
    "    if os.path.isfile(os.path.join(data_output_dir, (ba_to_process + '_Heat_Wave_Time_Series_' + str(start_year) + '_to_' + str(end_year) + '.csv'))) == True:\n",
    "       # Load in the pre-processed data:\n",
    "       met_df = pd.read_csv(os.path.join(data_output_dir, (ba_to_process + '_Heat_Wave_Time_Series_' + str(start_year) + '_to_' + str(end_year) + '.csv')))\n",
    "    else:\n",
    "       met_df = process_heat_wave_time_series(ba_to_process = ba_to_process, \n",
    "                                              start_year = start_year, \n",
    "                                              end_year = end_year, \n",
    "                                              base_year_start = base_year_start,\n",
    "                                              base_year_end = base_year_end,\n",
    "                                              weather_data_dir = weather_data_dir, \n",
    "                                              service_territory_data_dir = service_territory_data_dir, \n",
    "                                              population_data_dir = population_data_dir, \n",
    "                                              load_data_dir = load_data_dir, \n",
    "                                              data_output_dir = data_output_dir)\n",
    "    \n",
    "    # Set the time variable as an index:\n",
    "    met_df.index = pd.to_datetime(met_df['Time_UTC'])\n",
    "    \n",
    "    # Compute the daily minimum and maximum temperature using resampling:\n",
    "    uw_df = met_df.resample('D')['T2_UW'].agg(['min', 'max']).reset_index()\n",
    "    pw_df = met_df.resample('D')['T2_PW'].agg(['min', 'max']).reset_index()\n",
    "    uw_hw_df = met_df.resample('D')['UW_HW'].agg(['max']).reset_index()\n",
    "    pw_hw_df = met_df.resample('D')['PW_HW'].agg(['max']).reset_index()\n",
    "    joint_hw_df = met_df.resample('D')['Joint_HW'].agg(['max']).reset_index()\n",
    "        \n",
    "    # Rename the variables for consistency:\n",
    "    uw_df.rename(columns={'Time_UTC': 'Day', 'min': 'T2_UW_Min', 'max': 'T2_UW_Max'}, inplace=True)      \n",
    "    pw_df.rename(columns={'Time_UTC': 'Day', 'min': 'T2_PW_Min', 'max': 'T2_PW_Max'}, inplace=True)      \n",
    "    uw_hw_df.rename(columns={'Time_UTC': 'Day', 'max': 'UW_HW'}, inplace=True)  \n",
    "    pw_hw_df.rename(columns={'Time_UTC': 'Day', 'max': 'PW_HW'}, inplace=True)\n",
    "    joint_hw_df.rename(columns={'Time_UTC': 'Day', 'max': 'Joint_HW'}, inplace=True) \n",
    "   \n",
    "    # Merge the two dataframes together based on common days:\n",
    "    daily_df = uw_df.merge(pw_df, on=['Day']).merge(uw_hw_df, on=['Day']).merge(pw_hw_df, on=['Day']).merge(joint_hw_df, on=['Day'])\n",
    "    \n",
    "    # Calculate the number of heat wave days:\n",
    "    uw_hw_count = (len(daily_df.loc[daily_df['UW_HW'] == 1]))/40\n",
    "    pw_hw_count = (len(daily_df.loc[daily_df['PW_HW'] == 1]))/40\n",
    "    \n",
    "    print('Unweighted Heat Wave Days Per Year = ' + str(uw_hw_count))\n",
    "    print('Population-Weighted Heat Wave Days Per Year = ' + str(pw_hw_count))\n",
    "    \n",
    "    # Make the plot:\n",
    "    fig, ax = plt.subplots(2, figsize=(25, 15), sharex=True, sharey=True)\n",
    "    plt.rcParams['font.size'] = 18\n",
    "    ax[0].plot(daily_df['Day'], 200*daily_df['UW_HW'], 'r-', label=('Unweighted Heat Waves: N = ' + str(uw_hw_count) + ' Per Year'), linewidth=1)\n",
    "    ax[0].plot(daily_df['Day'], daily_df['T2_UW_Max'], 'k-', label='Unweighted T2 Max', linewidth=1)\n",
    "    ax[1].plot(daily_df['Day'], 200*daily_df['PW_HW'], 'r-', label=('Pop-Weighted Heat Waves: N = ' + str(pw_hw_count) + ' Per Year'), linewidth=1)\n",
    "    ax[1].plot(daily_df['Day'], daily_df['T2_PW_Max'], 'k-', label='Pop-Weighted T2 Max', linewidth=1)\n",
    "    plt.xlim(daily_df['Day'].min(), daily_df['Day'].max())\n",
    "    plt.ylim(daily_df[['T2_UW_Max', 'T2_PW_Max']].min().min(), daily_df[['T2_UW_Max', 'T2_PW_Max']].max().max())\n",
    "    ax[0].set_title((ba_to_process + ' Unweighted Daily Maximum Temperature: ' + str(start_year) + ' to ' + str(end_year)))\n",
    "    ax[1].set_title((ba_to_process + ' Population-Weighted Daily Maximum Temperature: ' + str(start_year) + ' to ' + str(end_year)))\n",
    "    ax[0].set_ylabel('Daily Maximum Temperature [$^\\circ$F]')\n",
    "    ax[1].set_ylabel('Daily Maximum Temperature [$^\\circ$F]')\n",
    "    ax[0].legend(loc='lower right', facecolor='white', framealpha=1)\n",
    "    ax[1].legend(loc='lower right', facecolor='white', framealpha=1)\n",
    "    \n",
    "    # If the \"save_images\" flag is set to true then save the plot to a .png file:\n",
    "    if save_images == True:\n",
    "       plt.savefig(os.path.join(image_output_dir, (ba_to_process + '_Heat_Wave_Time_Series_' + str(start_year) + '_to_' + str(end_year) + '.png')), \n",
    "                   dpi=image_resolution, bbox_inches='tight', facecolor='white')\n",
    "       plt.close()\n",
    "    \n",
    "    # Make a copy of the dataframe in order to add random noise:\n",
    "    noise_daily_df = daily_df.copy()\n",
    "    \n",
    "    # Add in random noise around the heat wave points:\n",
    "    noise_daily_df['UW_HW_Noise'] = noise_daily_df['UW_HW'] + np.random.normal(0, 0.05, [len(noise_daily_df)])\n",
    "    noise_daily_df['PW_HW_Noise'] = noise_daily_df['PW_HW'] + np.random.normal(0, 0.05, [len(noise_daily_df)])\n",
    "    \n",
    "    # Calculate the heat wave detection statistics:\n",
    "    no_no = noise_daily_df.loc[(noise_daily_df['UW_HW'] == 0) & (noise_daily_df['PW_HW'] == 0)]\n",
    "    yes_no = noise_daily_df.loc[(noise_daily_df['UW_HW'] == 1) & (noise_daily_df['PW_HW'] == 0)]\n",
    "    no_yes = noise_daily_df.loc[(daily_df['UW_HW'] == 0) & (noise_daily_df['PW_HW'] == 1)]\n",
    "    yes_yes = noise_daily_df.loc[(daily_df['UW_HW'] == 1) & (noise_daily_df['PW_HW'] == 1)]\n",
    "    \n",
    "    # Make the plot:\n",
    "    plt.figure(figsize=(25, 15))\n",
    "    plt.rcParams['font.size'] = 18\n",
    "    plt.scatter(no_no['UW_HW_Noise'], no_no['PW_HW_Noise'], s=15, c='black', label=('No Unweighted HW, No Pop-Weighted HW, N = ' + str(len(no_no)) + ' Days'))\n",
    "    plt.scatter(yes_no['UW_HW_Noise'], yes_no['PW_HW_Noise'], s=15, c='green', label=('Unweighted HW, No Pop-Weighted HW, N = ' + str(len(yes_no)) + ' Days'))\n",
    "    plt.scatter(no_yes['UW_HW_Noise'], no_yes['PW_HW_Noise'], s=15, c='blue', label=('No Unweighted HW, Pop-Weighted HW, N = ' + str(len(no_yes)) + ' Days'))\n",
    "    plt.scatter(yes_yes['UW_HW_Noise'], yes_yes['PW_HW_Noise'], s=15, c='red', label=('Unweighted HW, Pop-Weighted HW, N = ' + str(len(yes_yes)) + ' Days'))\n",
    "    plt.grid()\n",
    "    plt.xlim(-0.25, 1.25)\n",
    "    plt.ylim(-0.25, 1.25)\n",
    "    plt.xlabel('Unweighted Heat Wave Detected: 0 = No, 1 = Yes')\n",
    "    plt.ylabel('Population-Weighted Heat Wave Detected: 0 = No, 1 = Yes')\n",
    "    plt.legend(loc='center', facecolor='white', framealpha=1)\n",
    "    plt.title('Heat Wave Detection Frequency in ' + ba_to_process + ' from ' + str(start_year) + ' to ' + str(end_year))\n",
    "    \n",
    "    # If the \"save_images\" flag is set to true then save the plot to a .png file:\n",
    "    if save_images == True:\n",
    "       plt.savefig(os.path.join(image_output_dir, (ba_to_process + '_Heat_Wave_Detection_' + str(start_year) + '_to_' + str(end_year) + '.png')), \n",
    "                   dpi=image_resolution, bbox_inches='tight', facecolor='white')\n",
    "       plt.close()\n",
    "    \n",
    "    return daily_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e754e920-6489-41cd-9170-00bd9dce9ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df = plot_heat_wave_frequency(ba_to_process = 'ERCO', \n",
    "                                     start_year = 1980, \n",
    "                                     end_year = 2020, \n",
    "                                     base_year_start = 1980,\n",
    "                                     base_year_end = 1990,\n",
    "                                     weather_data_dir = weather_data_dir, \n",
    "                                     service_territory_data_dir = service_territory_data_dir, \n",
    "                                     population_data_dir = population_data_dir, \n",
    "                                     load_data_dir = load_data_dir, \n",
    "                                     data_output_dir = data_output_dir,\n",
    "                                     image_output_dir = image_output_dir, \n",
    "                                     image_resolution = 300, \n",
    "                                     save_images = True)\n",
    "\n",
    "output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ca8c9a-5069-40d7-a2d3-4f3f6f00e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heat_wave_characteristics(ba_to_process: str, start_year: int, end_year: int, base_year_start: int, base_year_end: int,\n",
    "                                   weather_data_dir: str, service_territory_data_dir: str, \n",
    "                                   population_data_dir: str, load_data_dir: str, data_output_dir: str,\n",
    "                                   image_output_dir: str, image_resolution: int, save_images=False):\n",
    "    \n",
    "    # Check to see if the output already exist and if not then process it:\n",
    "    if os.path.isfile(os.path.join(data_output_dir, (ba_to_process + '_Heat_Wave_Time_Series_' + str(start_year) + '_to_' + str(end_year) + '.csv'))) == True:\n",
    "       # Load in the pre-processed data:\n",
    "       met_df = pd.read_csv(os.path.join(data_output_dir, (ba_to_process + '_Heat_Wave_Time_Series_' + str(start_year) + '_to_' + str(end_year) + '.csv')))\n",
    "    else:\n",
    "       met_df = process_heat_wave_time_series(ba_to_process = ba_to_process, \n",
    "                                              start_year = start_year, \n",
    "                                              end_year = end_year, \n",
    "                                              base_year_start = base_year_start,\n",
    "                                              base_year_end = base_year_end,\n",
    "                                              weather_data_dir = weather_data_dir, \n",
    "                                              service_territory_data_dir = service_territory_data_dir, \n",
    "                                              population_data_dir = population_data_dir, \n",
    "                                              load_data_dir = load_data_dir, \n",
    "                                              data_output_dir = data_output_dir)\n",
    "    \n",
    "    # Subset to just the heat wave days:\n",
    "    uw_hw = met_df.loc[(met_df['UW_HW'] == 1)]\n",
    "    pw_hw = met_df.loc[(met_df['PW_HW'] == 1)]\n",
    "    \n",
    "    # Make the plot:\n",
    "    plt.figure(figsize=(25, 15))\n",
    "    plt.rcParams['font.size'] = 18\n",
    "    plt.subplot(221)\n",
    "    plt.hist(uw_hw['T2_UW'], bins=25, density=True, histtype='step', edgecolor = 'r', label=('Mean=' + str(uw_hw['T2_UW'].mean().round(1)) + '$^\\circ$F, Max=' + str(uw_hw['T2_UW'].max().round(1)) + '$^\\circ$F'), linewidth=3)\n",
    "    plt.legend(loc='upper center', facecolor='white', framealpha=1)\n",
    "    plt.title('Unweighted HW Temperatures in ' + ba_to_process)\n",
    "    \n",
    "    plt.subplot(222)\n",
    "    plt.hist(uw_hw['Load_MWh'], bins=25, density=True, histtype='step', edgecolor = 'b', label=('Mean=' + str(uw_hw['Load_MWh'].mean().round(1)) + ' MWh, Max=' + str(uw_hw['Load_MWh'].max().round(1)) + ' MWh'), linewidth=3)\n",
    "    plt.legend(loc='upper center', facecolor='white', framealpha=1)\n",
    "    plt.title('Unweighted HW Loads in ' + ba_to_process)\n",
    "    \n",
    "    plt.subplot(223)\n",
    "    plt.hist(pw_hw['T2_PW'], bins=25, density=True, histtype='step', edgecolor = 'r', label=('Mean=' + str(pw_hw['T2_PW'].mean().round(1)) + '$^\\circ$F, Max=' + str(pw_hw['T2_PW'].max().round(1)) + '$^\\circ$F'), linewidth=3)\n",
    "    plt.legend(loc='upper center', facecolor='white', framealpha=1)\n",
    "    plt.title('Pop-Weighted HW Temperatures in ' + ba_to_process)\n",
    "    \n",
    "    plt.subplot(224)\n",
    "    plt.hist(pw_hw['Load_MWh'], bins=25, density=True, histtype='step', edgecolor = 'b', label=('Mean=' + str(pw_hw['Load_MWh'].mean().round(1)) + ' MWh, Max=' + str(pw_hw['Load_MWh'].max().round(1)) + ' MWh'), linewidth=3)\n",
    "    plt.legend(loc='upper center', facecolor='white', framealpha=1)\n",
    "    plt.title('Pop-Weighted HW Loads in ' + ba_to_process)\n",
    "    \n",
    "    # If the \"save_images\" flag is set to true then save the plot to a .png file:\n",
    "    if save_images == True:\n",
    "       plt.savefig(os.path.join(image_output_dir, (ba_to_process + '_Heat_Wave_T_and_Load_Distributions_' + str(start_year) + '_to_' + str(end_year) + '.png')), \n",
    "                   dpi=image_resolution, bbox_inches='tight', facecolor='white')\n",
    "       plt.close()\n",
    "        \n",
    "    return pw_hw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e995951-8032-44aa-9576-fb983154099b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df = plot_heat_wave_characteristics(ba_to_process = 'CISO', \n",
    "                                           start_year = 1980, \n",
    "                                           end_year = 2020, \n",
    "                                           base_year_start = 1980,\n",
    "                                           base_year_end = 1990,\n",
    "                                           weather_data_dir = weather_data_dir, \n",
    "                                           service_territory_data_dir = service_territory_data_dir, \n",
    "                                           population_data_dir = population_data_dir, \n",
    "                                           load_data_dir = load_data_dir, \n",
    "                                           data_output_dir = data_output_dir,\n",
    "                                           image_output_dir = image_output_dir, \n",
    "                                           image_resolution = 300, \n",
    "                                           save_images = True)\n",
    "\n",
    "output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5d3fc4-280f-433c-8c07-d893a1b79602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_uw_pw_t_distributions(ba_to_process: str, start_year: int, end_year: int, base_year_start: int, base_year_end: int,\n",
    "                               weather_data_dir: str, service_territory_data_dir: str, \n",
    "                               population_data_dir: str, load_data_dir: str, data_output_dir: str,\n",
    "                               image_output_dir: str, image_resolution: int, save_images=False):\n",
    "    \n",
    "    # Check to see if the output already exist and if not then process it:\n",
    "    if os.path.isfile(os.path.join(data_output_dir, (ba_to_process + '_Heat_Wave_Time_Series_' + str(start_year) + '_to_' + str(end_year) + '.csv'))) == True:\n",
    "       # Load in the pre-processed data:\n",
    "       met_df = pd.read_csv(os.path.join(data_output_dir, (ba_to_process + '_Heat_Wave_Time_Series_' + str(start_year) + '_to_' + str(end_year) + '.csv')))\n",
    "    else:\n",
    "       met_df = process_heat_wave_time_series(ba_to_process = ba_to_process, \n",
    "                                              start_year = start_year, \n",
    "                                              end_year = end_year, \n",
    "                                              base_year_start = base_year_start,\n",
    "                                              base_year_end = base_year_end,\n",
    "                                              weather_data_dir = weather_data_dir, \n",
    "                                              service_territory_data_dir = service_territory_data_dir, \n",
    "                                              population_data_dir = population_data_dir, \n",
    "                                              load_data_dir = load_data_dir, \n",
    "                                              data_output_dir = data_output_dir)\n",
    "    one_to_one = np.arange(0, 1000, 100)\n",
    "        \n",
    "    # Make the plot:\n",
    "    plt.figure(figsize=(25, 15))\n",
    "    plt.rcParams['font.size'] = 18\n",
    "    plt.scatter(met_df['T2_UW'], met_df['T2_PW'], s=10, facecolors='none', edgecolors='b', label=('Test'))\n",
    "    plt.plot(one_to_one, one_to_one, 'k', linewidth=3, label = '1:1')\n",
    "    plt.xlim(0, 120)\n",
    "    plt.ylim(0, 120)\n",
    "    plt.xlabel('Unweighted Temperature')\n",
    "    plt.ylabel('Population-Weighted Temperature')\n",
    "    plt.title('Weighting Effect in ' + ba_to_process + ' from ' + str(start_year) + ' to ' + str(end_year))\n",
    "    \n",
    "    # If the \"save_images\" flag is set to true then save the plot to a .png file:\n",
    "    if save_images == True:\n",
    "       plt.savefig(os.path.join(image_output_dir, (ba_to_process + '_Weighting_Effect_' + str(start_year) + '_to_' + str(end_year) + '.png')), \n",
    "                   dpi=image_resolution, bbox_inches='tight', facecolor='white')\n",
    "       plt.close()\n",
    "        \n",
    "    return met_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef546e-09f5-46c1-bc1c-b378ef23a49b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df = plot_uw_pw_t_distributions(ba_to_process = 'CISO', \n",
    "                                       start_year = 1980, \n",
    "                                       end_year = 2020, \n",
    "                                       base_year_start = 1980,\n",
    "                                       base_year_end = 1990,\n",
    "                                       weather_data_dir = weather_data_dir, \n",
    "                                       service_territory_data_dir = service_territory_data_dir, \n",
    "                                       population_data_dir = population_data_dir, \n",
    "                                       load_data_dir = load_data_dir, \n",
    "                                       data_output_dir = data_output_dir,\n",
    "                                       image_output_dir = image_output_dir, \n",
    "                                       image_resolution = 300, \n",
    "                                       save_images = True)\n",
    "\n",
    "output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a067f863-ae78-4b7e-9462-dfc8bbd9f854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9.15_std",
   "language": "python",
   "name": "py3.9.15_std"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
